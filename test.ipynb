{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nba055\\.conda\\envs\\tensorflow2\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to ~\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:01,  2.84 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.84 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.84 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.84 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.84 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.84 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  2.84 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  7.23 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  7.23 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  7.23 url/s]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:01<00:00,  3.32 file/s]\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  8.29 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  3.31 url/s]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to ~\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    }
   ],
   "source": [
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\", \"/gpu:2\", \"/gpu:3\", \"/gpu:4\", \"/gpu:5\", \"/gpu:6\", \"/gpu:7\"], cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 8\n"
     ]
    }
   ],
   "source": [
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also do info.splits.total_num_examples to get the total\n",
    "# number of examples in the dataset.\n",
    "\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image /= 255\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints.\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Define the name of the checkpoint files.\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for decaying the learning rate.\n",
    "# You can define any decay function you need.\n",
    "def decay(epoch):\n",
    "  if epoch < 3:\n",
    "    return 1e-3\n",
    "  elif epoch >= 3 and epoch < 7:\n",
    "    return 1e-4\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback for printing the learning rate at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    print('\\nLearning rate for epoch {} is {}'.format(        epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the callbacks together.\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = hierarchical_copy, num_packs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: 0.1099\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "118/118 [==============================] - 28s 65ms/step - loss: nan - accuracy: 0.1099 - lr: 0.0010\n",
      "Epoch 2/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: 0.8869\n",
      "Learning rate for epoch 2 is 0.0010000000474974513\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: 0.8869 - lr: 0.0010\n",
      "Epoch 3/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 3 is 0.0010000000474974513\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 0.0010\n",
      "Epoch 4/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 4 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-04\n",
      "Epoch 5/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 5 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-04\n",
      "Epoch 6/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 6 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-04\n",
      "Epoch 7/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 7 is 9.999999747378752e-05\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-04\n",
      "Epoch 8/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 8 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05\n",
      "Epoch 9/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 9 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05\n",
      "Epoch 10/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 10 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05\n",
      "Epoch 11/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 11 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05\n",
      "Epoch 12/12\n",
      "118/118 [==============================] - ETA: 0s - loss: nan - accuracy: nan\n",
      "Learning rate for epoch 12 is 9.999999747378752e-06\n",
      "118/118 [==============================] - 8s 65ms/step - loss: nan - accuracy: nan - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x169e86932b0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 12\n",
    "\n",
    "model.fit(train_dataset, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "932d676561e9b164b788fdd61d3fff2923b45187487f00516d1ccbb1fe4d7f43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
